{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnNoWsBJ5n4tqZuxEzCReM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavbajpai79/AIML/blob/main/SLM__with_RAG_for_financial_statements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb4gZ6xNco0B"
      },
      "outputs": [],
      "source": [
        "\"\"\"SLM with RAG for financial statements\"\"\"\n",
        "\n",
        "# Develop a Retrieval-Augmented Generation (RAG) model\n",
        "# to answer financial questions based on company financial statements (last two years).\n",
        "\n",
        "# requirements for the application\n",
        "# numpy\n",
        "# pandas\n",
        "# matplotlib\n",
        "# torch\n",
        "# transformers\n",
        "# sentence_transformers\n",
        "# spacy\n",
        "# faiss-cpu\n",
        "# pdfplumber\n",
        "# rank_bm25\n",
        "# fastapi\n",
        "# gradio\n",
        "\n",
        "# Importing the dependencies\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import spacy\n",
        "import pdfplumber\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "# Initialize logger\n",
        "logging.basicConfig(\n",
        "    # filename=\"app.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# SLM: Microsoft PHI-2 model is loaded\n",
        "# It does have higher memory and compute requirements compared to TinyLlama and Falcon\n",
        "# But it gives the best results among the three\n",
        "# DEVICE = \"cpu\"  # or cuda\n",
        "DEVICE = \"cuda\"  # or cpu\n",
        "# MODEL_NAME = \"TinyLlama/TinyLlama_v1.1\"\n",
        "# MODEL_NAME = \"tiiuae/falcon-rw-1b\"\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "# MODEL_NAME = \"google/gemma-3-1b-pt\"\n",
        "# Load the Tokenizer for PHI-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "MAX_TOKENS = tokenizer.model_max_length\n",
        "CONTEXT_MULTIPLIER = 0.7\n",
        "# The max_context tokens is used to limit the retrieved chunks during querying\n",
        "# to provide some headroom for the query\n",
        "MAX_CONTEXT_TOKENS = int(MAX_TOKENS * CONTEXT_MULTIPLIER)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# Since the model is to be hosted on a cpu instance, we use float32\n",
        "# For GPU, we can use float16 or bfloat16\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
        ").to(DEVICE)\n",
        "model.eval()\n",
        "# model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "logger.info(\"Model loaded successfully.\")\n",
        "# Load Sentence Transformer for Embeddings and Cross Encoder for re-ranking\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
        "# Load spaCy English model for Named Entity Recognition (mainly for guardrail)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the regex patterns, words blacklist\n",
        "restricted_patterns = [\n",
        "    r\"\\b(?:cfo|ceo|cto|executive|director|manager|employee|staff|worker)\\b.*\\b(?:salary|compensation|bonus|pay|income)\\b\",\n",
        "    r\"\\b(?:salary|compensation|bonus|pay|income)\\b.*\\b(?:cfo|ceo|cto|executive|director|manager|employee|staff|worker)\\b\",\n",
        "    r\"\\b(?:acquisition|merger|buyout)\\b.*\\b(?:before|pre-announcement|leak|inside information)\\b\",\n",
        "    r\"\\b(?:before|pre-announcement|leak|inside information)\\b.*\\b(?:acquisition|merger|buyout)\\b\",\n",
        "    r\"\\b(?:stock price|share price|insider trading|buying shares)\\b\",\n",
        "    r\"\\b(?:internal policy|data breach|security protocol|confidential|classified)\\b\",\n",
        "    r\"\\b(?:password|access credentials|encryption key|secure key)\\b\",\n",
        "    r\"\\b(?:social security number|ssn|passport number|credit card|bank account|tax id|tin|personal details)\\b\",\n",
        "    r\"\\b(?:employee records|payroll|medical records|hr data|salary data|pii|personally identifiable information)\\b\",\n",
        "    r\"\\b(?:cfo|ceo|cto|executive|director|manager|employee|staff|worker)\\b.*\\b(?:address|work location|home location|residence|personal contact|phone number|email|office location)\\b\",\n",
        "]\n",
        "\n",
        "restricted_topics = {\n",
        "    \"CEO salary\",\n",
        "    \"CFO salary\",\n",
        "    \"executive pay\",\n",
        "    \"stock options\",\n",
        "    \"compensation details\",\n",
        "    \"classified financial data\",\n",
        "    \"insider trading\",\n",
        "    \"password\",\n",
        "    \"login credentials\",\n",
        "    \"HR complaints\",\n",
        "    \"remuneration\",\n",
        "    \"director salary\",\n",
        "    \"financial package\",\n",
        "}\n",
        "\n",
        "FINANCIAL_ENTITY_LABELS = {\"MONEY\", \"PERCENT\", \"CARDINAL\", \"ORG\"}\n",
        "\n",
        "GENERAL_KNOWLEDGE_PATTERNS = [\n",
        "    r\"\\b(?:capital of|where is|who is|when did|what is|history of|define|meaning of|synonym of|antonym of|explain|how does|why is)\\b\",\n",
        "    r\"\\b(?:country|city|continent|leader|president|prime minister|language|currency|population|politics|war|anthem|flag|national animal|national bird|national flower|national sport|monarch|king|queen|ruler|army|military|constitution|government|laws|famous person|historical figure|famous landmark|ocean|mountain|river|lake|climate|weather|culture|tradition|festival|holiday|invention|discovery|science|technology|art|literature|music|religion|mythology|folklore|education|university|school|mathematics|physics|chemistry|biology|philosophy|astronomy|space|planet|star|galaxy|universe|health|medicine|disease|virus|bacteria|genetics|DNA|evolution|ecology|environment|pollution|wildlife|habitat|natural disaster|earthquake|volcano|tsunami|hurricane|storm|flood|drought)\\b\",\n",
        "    r\"\\b(?:[A-Z][a-z]+(?:'s)?\\s+(?:capital|president|prime minister|national animal|national bird|national flower|national sport|anthem|flag|currency|language|leader|government|constitution|laws|monarch|king|queen|army|military|famous person|historical figure|landmark|river|ocean|mountain|religion|festival|holiday))\\b\",\n",
        "]\n",
        "\n",
        "\n",
        "sensitive_terms = {\n",
        "    \"salary\",\n",
        "    \"compensation\",\n",
        "    \"income\",\n",
        "    \"pay\",\n",
        "    \"bonus\",\n",
        "    \"earnings\",\n",
        "    \"wages\",\n",
        "}\n",
        "\n",
        "EXPLANATORY_PATTERNS = [\n",
        "    r\"\\b(why|reason|cause|explanation|due to|because|factor|impact of|effect of|influence of|driven by)\\b\",\n",
        "    r\"\\b(how did|what led to|what caused|why did|how was|contributing factor|explain)\\b\",\n",
        "]\n",
        "\n",
        "FINANCIAL_DATA_PATTERNS = (\n",
        "    r\"\\b(\\â‚¹?\\s?\\d{1,3}(?:,\\d{2,3})*(?:\\.\\d+)?\\s*(million|billion|crore|lakh|%)\"\n",
        "    r\"?|Rs\\.?\\s?\\d{1,3}(?:,\\d{2,3})*(?:\\.\\d+)?)\\b\"\n",
        ")\n",
        "\n",
        "FINANCIAL_TERMS = {\n",
        "    \"income\",\n",
        "    \"revenue\",\n",
        "    \"profit\",\n",
        "    \"dividend\",\n",
        "    \"investment\",\n",
        "    \"earnings\",\n",
        "    \"turnover\",\n",
        "    \"expenses\",\n",
        "    \"assets\",\n",
        "    \"liabilities\",\n",
        "    \"capital\",\n",
        "    \"cash\",\n",
        "    \"EBITDA\",\n",
        "    \"margin\",\n",
        "    \"tax\",\n",
        "    \"costs\",\n",
        "    \"reserves\",\n",
        "    \"equity\",\n",
        "    \"debt\",\n",
        "    \"interest\",\n",
        "    \"valuation\",\n",
        "    \"amortization\",\n",
        "    \"depreciation\",\n",
        "    \"returns\",\n",
        "    \"funds\",\n",
        "    \"shares\",\n",
        "    \"stock\",\n",
        "    \"pricing\",\n",
        "    \"liquidity\",\n",
        "    \"credit\",\n",
        "    \"bond\",\n",
        "    \"expense\",\n",
        "    \"budget\",\n",
        "    \"yield\",\n",
        "    \"growth\",\n",
        "}\n",
        "\n",
        "\n",
        "# Extract the yaer from the upload file's name if any\n",
        "def extract_year_from_filename(filename):\n",
        "    \"\"\"Extract Year from Filename\"\"\"\n",
        "    match = re.search(r\"(\\d{4})-(\\d{4})\", filename)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    match = re.search(r\"(\\d{4})\", filename)\n",
        "    return match.group(1) if match else \"Unknown\"\n",
        "\n",
        "\n",
        "# Use PDFPlumber to extract the tables from the uploaded file\n",
        "# Add the year column for context and create a dataframe\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    \"\"\"Extract tables from PDF into a DataFrame\"\"\"\n",
        "    all_tables = []\n",
        "    report_year = extract_year_from_filename(pdf_path)\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_num, page in enumerate(pdf.pages, start=1):\n",
        "            tables = page.extract_tables()\n",
        "            for table in tables:\n",
        "                df = pd.DataFrame(table)\n",
        "                df[\"year\"] = report_year\n",
        "                all_tables.append(df)\n",
        "    return pd.concat(all_tables, ignore_index=True) if all_tables else pd.DataFrame()\n",
        "\n",
        "\n",
        "# Load the csv files directly using pandas into a dataframe\n",
        "def load_csv(file_path):\n",
        "    \"\"\"Loads a CSV file into a DataFrame\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        df[\"year\"] = extract_year_from_filename(file_path)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Preprocess the dataframe - Replace null values and create text rows suitable for chunking\n",
        "def clean_dataframe_text(df):\n",
        "    \"\"\"Clean and format PDF/CSV data\"\"\"\n",
        "    df.fillna(\"\", inplace=True)\n",
        "    text_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        parts = []\n",
        "        if \"year\" in df.columns:\n",
        "            parts.append(f\"Year: {row['year']}\")\n",
        "        parts.extend([str(val).strip() for val in row if str(val).strip()])\n",
        "        text_data.append(\", \".join(parts))\n",
        "    df[\"text\"] = text_data\n",
        "    return df[[\"text\"]].replace(\"\", np.nan).dropna()\n",
        "\n",
        "\n",
        "# Chunk the text for retrival\n",
        "# Different chunk sizes - 256,512,1024,2048 were tried and 512 worked the best for financial RAG\n",
        "def chunk_text(text, chunk_size=512):\n",
        "    \"\"\"Apply Chunking on the text\"\"\"\n",
        "    words = text.split()\n",
        "    chunks, temp_chunk = [], []\n",
        "    for word in words:\n",
        "        if sum(len(w) for w in temp_chunk) + len(temp_chunk) + len(word) <= chunk_size:\n",
        "            temp_chunk.append(word)\n",
        "        else:\n",
        "            chunks.append(\" \".join(temp_chunk))\n",
        "            temp_chunk = [word]\n",
        "    if temp_chunk:\n",
        "        chunks.append(\" \".join(temp_chunk))\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Uses regex to identify financial terms and ensure relevant data is only merged\n",
        "def is_financial_text(text):\n",
        "    \"\"\"Detects financial data\"\"\"\n",
        "    return bool(\n",
        "        re.search(\n",
        "            FINANCIAL_DATA_PATTERNS,\n",
        "            text,\n",
        "            re.IGNORECASE,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Advanced RAG - Chunk Merging\n",
        "# Uses a sentence transformer \"all-MiniLM-L6-v2\" to embed text chunks\n",
        "# Stores embeddings in a FAISS vector database for similarity search\n",
        "# BM25 is implemented alongside FAISS to improve retrieval\n",
        "# Use FAISS Cosine Similarity index and merge only highly similar text chunks (>85%)\n",
        "def merge_similar_chunks(chunks, similarity_threshold=0.85):\n",
        "    \"\"\"Merge similar chunks while preserving financial data structure\"\"\"\n",
        "    if not chunks:\n",
        "        return []\n",
        "    # Encode chunks into embeddings\n",
        "    embeddings = np.array(\n",
        "        embed_model.encode(chunks, normalize_embeddings=True), dtype=\"float32\"\n",
        "    )\n",
        "    # FAISS Cosine Similarity Index\n",
        "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    # Get top-2 most similar chunks\n",
        "    _, indices = index.search(embeddings, 2)\n",
        "    merged_chunks = {}\n",
        "    for i, idx in enumerate(indices[:, 1]):\n",
        "        if i in merged_chunks or idx in merged_chunks:\n",
        "            continue\n",
        "        sim_score = np.dot(embeddings[i], embeddings[idx])\n",
        "        # Ensure financial data isn't incorrectly merged\n",
        "        if is_financial_text(chunks[i]) or is_financial_text(chunks[idx]):\n",
        "            merged_chunks[i] = chunks[i]\n",
        "            merged_chunks[idx] = chunks[idx]\n",
        "            continue\n",
        "        # Merge only if similarity is high and chunks are adjacent\n",
        "        if sim_score > similarity_threshold and abs(i - idx) == 1:\n",
        "            merged_chunks[i] = chunks[i] + \" \" + chunks[idx]\n",
        "            merged_chunks[idx] = merged_chunks[i]\n",
        "        else:\n",
        "            merged_chunks[i] = chunks[i]\n",
        "    return list(set(merged_chunks.values()))\n",
        "\n",
        "\n",
        "# Handle for file upload button in UI\n",
        "# Processes the uploaded files and generates the embeddings\n",
        "# The FAISS embeddings and tokenized chunks are saved for retrieval\n",
        "def process_files(files, chunk_size=512):\n",
        "    \"\"\"Process uploaded files and generate embeddings\"\"\"\n",
        "    if not files:\n",
        "        logger.warning(\"No files uploaded!\")\n",
        "        return \"Please upload at least one PDF or CSV file.\"\n",
        "    pdf_paths = [file.name for file in files if file.name.endswith(\".pdf\")]\n",
        "    csv_paths = [file.name for file in files if file.name.endswith(\".csv\")]\n",
        "    logger.info(f\"Processing {len(pdf_paths)} PDFs and {len(csv_paths)} CSVs\")\n",
        "    df_list = []\n",
        "    if pdf_paths:\n",
        "        df_list.extend([extract_tables_from_pdf(pdf) for pdf in pdf_paths])\n",
        "    for csv in csv_paths:\n",
        "        df = load_csv(csv)\n",
        "        df_list.append(df)\n",
        "    if not df_list:\n",
        "        logger.warning(\"No valid data found in the uploaded files\")\n",
        "        return \"No valid data found in the uploaded files\"\n",
        "    df = pd.concat(df_list, ignore_index=True)\n",
        "    df.dropna(how=\"all\", inplace=True)\n",
        "    logger.info(\"Data extracted from the files\")\n",
        "    df_cleaned = clean_dataframe_text(df)\n",
        "    df_cleaned[\"chunks\"] = df_cleaned[\"text\"].apply(lambda x: chunk_text(x, chunk_size))\n",
        "    df_chunks = df_cleaned.explode(\"chunks\").reset_index(drop=True)\n",
        "    merged_chunks = merge_similar_chunks(df_chunks[\"chunks\"].tolist())\n",
        "    chunk_texts = merged_chunks\n",
        "    # chunk_texts = df_chunks[\"chunks\"].tolist()\n",
        "    embeddings = np.array(\n",
        "        embed_model.encode(chunk_texts, normalize_embeddings=True), dtype=\"float32\"\n",
        "    )\n",
        "    # Save FAISS index\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, \"data/faiss_index.bin\")\n",
        "    logger.info(\"FAISS index created and saved.\")\n",
        "    # Save BM25 index\n",
        "    tokenized_chunks = [text.lower().split() for text in chunk_texts]\n",
        "    bm25_data = {\"tokenized_chunks\": tokenized_chunks, \"chunk_texts\": chunk_texts}\n",
        "    logger.info(\"BM25 index created and saved.\")\n",
        "    with open(\"data/bm25_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(bm25_data, f)\n",
        "    return \"Files processed successfully! You can now query.\"\n",
        "\n",
        "\n",
        "def contains_financial_entities(query):\n",
        "    \"\"\"Check if query contains financial entities\"\"\"\n",
        "    doc = nlp(query)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in FINANCIAL_ENTITY_LABELS:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def contains_geographical_entities(query):\n",
        "    \"\"\"Check if the query contains geographical entities\"\"\"\n",
        "    doc = nlp(query)\n",
        "    return any(ent.label_ == \"GPE\" for ent in doc.ents)\n",
        "\n",
        "\n",
        "def contains_financial_terms(query):\n",
        "    \"\"\"Check if the query contains financial terms\"\"\"\n",
        "    return any(term in query.lower() for term in FINANCIAL_TERMS)\n",
        "\n",
        "\n",
        "def is_general_knowledge_query(query):\n",
        "    \"\"\"Check if query contains general knowledge\"\"\"\n",
        "    query_lower = query.lower()\n",
        "    for pattern in GENERAL_KNOWLEDGE_PATTERNS:\n",
        "        if re.search(pattern, query_lower):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_latest_available_year(retrieved_chunks):\n",
        "    \"\"\"Extracts the latest available year from retrieved financial data\"\"\"\n",
        "    years = set()\n",
        "    year_pattern = r\"\\b(20\\d{2})\\b\"\n",
        "    for chunk in retrieved_chunks:\n",
        "        years.update(map(int, re.findall(year_pattern, chunk)))\n",
        "    return max(years) if years else 2024\n",
        "\n",
        "\n",
        "def is_irrelevant_query(query):\n",
        "    \"\"\"Check if the query is not finance related\"\"\"\n",
        "    # If the query is general knowledge and not finance-related\n",
        "    if is_general_knowledge_query(query) and not contains_financial_terms(query):\n",
        "        return True\n",
        "    # If the query contains only geographical terms without financial entities\n",
        "    if contains_geographical_entities(query) and not contains_financial_entities(query):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Input guardrail implementation\n",
        "# NER + Regex + List of terms used to filter irrelevant queries\n",
        "# Regex is used to filter queries related to sensitive topics\n",
        "# Uses spaCy model's Named Entity Recognition to filter queries for personal details\n",
        "# Uses cosine similarity with the embedded query and sensitive topic vectors\n",
        "# to filter out queries violating confidential/security rules (additional)\n",
        "def is_query_allowed(query):\n",
        "    \"\"\"Checks if the query violates security or confidentiality rules\"\"\"\n",
        "    if is_irrelevant_query(query):\n",
        "        return False, \"Query is not finance-related. Please ask a financial question.\"\n",
        "    for pattern in restricted_patterns:\n",
        "        if re.search(pattern, query.lower(), re.IGNORECASE):\n",
        "            return False, \"This query requests sensitive or confidential information.\"\n",
        "    doc = nlp(query)\n",
        "    # Check if there's a person entity and contains sensitive terms\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            for token in ent.subtree:\n",
        "                if token.text.lower() in sensitive_terms:\n",
        "                    return (\n",
        "                        False,\n",
        "                        \"Query contains personal salary information, which is restricted.\",\n",
        "                    )\n",
        "    query_embedding = embed_model.encode(query, normalize_embeddings=True)\n",
        "    topic_embeddings = embed_model.encode(\n",
        "        list(restricted_topics), normalize_embeddings=True\n",
        "    )\n",
        "    # Check similarities between the restricted topics and the query\n",
        "    similarities = np.dot(topic_embeddings, query_embedding)\n",
        "    if np.max(similarities) > 0.85:\n",
        "        return False, \"This query requests sensitive or confidential information.\"\n",
        "    return True, None\n",
        "\n",
        "\n",
        "# Boosts the scores for texts containing financial terms\n",
        "# This is useful during re-ranking\n",
        "def boost_score(text, base_score, boost_factor=1.2):\n",
        "    \"\"\"Boost scores if the text contains financial terms\"\"\"\n",
        "    if any(term in text.lower() for term in FINANCIAL_TERMS):\n",
        "        return base_score * boost_factor\n",
        "    return base_score\n",
        "\n",
        "\n",
        "# Advanced RAG - Adaptive Retrieval\n",
        "# FAISS embeddings are used to retrieve semantically similar chunks\n",
        "# BM25 is used to retrieve relevant chunks based on the keywords (TF-IDF)\n",
        "# FAISS and BM25 complement each other- similar matches and important exact matches\n",
        "# The retrieved chunks are merged and sorted based on a lambda FAISS value\n",
        "# if lambda FAISS is 0.6, weightage for retrieved FAISS chunks are 0.6 and 0.4 for BM25 chunks\n",
        "# Cross encoder model ms-marco-MiniLM-L6-v2 is used for scoring and re-ranking the chunks\n",
        "def hybrid_retrieve(query, chunk_texts, index, bm25, top_k=5, lambda_faiss=0.7):\n",
        "    \"\"\"Hybrid Retrieval with FAISS, BM25, Cross-Encoder & Financial Term Boosting\"\"\"\n",
        "    # FAISS Retrieval\n",
        "    query_embedding = np.array(\n",
        "        [embed_model.encode(query, normalize_embeddings=True)], dtype=\"float32\"\n",
        "    )\n",
        "    _, faiss_indices = index.search(query_embedding, top_k)\n",
        "    faiss_results = [chunk_texts[idx] for idx in faiss_indices[0]]\n",
        "    # BM25 Retrieval\n",
        "    tokenized_query = query.lower().split()\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "    bm25_top_indices = np.argsort(bm25_scores)[::-1][:top_k]\n",
        "    bm25_results = [chunk_texts[idx] for idx in bm25_top_indices]\n",
        "    # Merge FAISS & BM25 Scores\n",
        "    results = {}\n",
        "    for entry in faiss_results:\n",
        "        results[entry] = boost_score(entry, lambda_faiss)\n",
        "    for entry in bm25_results:\n",
        "        results[entry] = results.get(entry, 0) + boost_score(entry, (1 - lambda_faiss))\n",
        "    # Rank initial results\n",
        "    retrieved_docs = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    retrieved_texts = [r[0] for r in retrieved_docs]\n",
        "    # Cross-Encoder Re-Ranking\n",
        "    query_text_pairs = [[query, text] for text in retrieved_texts]\n",
        "    scores = cross_encoder.predict(query_text_pairs)\n",
        "    ranked_indices = np.argsort(scores)[::-1]\n",
        "    # Return top-ranked results\n",
        "    final_results = [retrieved_texts[i] for i in ranked_indices[:top_k]]\n",
        "    return final_results\n",
        "\n",
        "\n",
        "def compute_entropy(logits):\n",
        "    \"\"\"Compute entropy from logits.\"\"\"\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    log_probs = torch.log(probs + 1e-9)\n",
        "    entropy = -(probs * log_probs).sum(dim=-1)\n",
        "    return entropy.mean().item()\n",
        "\n",
        "\n",
        "def contains_future_year(query, retrieved_chunks):\n",
        "    \"\"\"Detects if the query asks for future data beyond available reports\"\"\"\n",
        "    latest_year = get_latest_available_year(retrieved_chunks)\n",
        "    # Extract years from query\n",
        "    future_years = set(map(int, re.findall(r\"\\b(20\\d{2})\\b\", query)))\n",
        "    return any(year > latest_year for year in future_years)\n",
        "\n",
        "\n",
        "def is_explanatory_query(query):\n",
        "    \"\"\"Checks if the query requires an explanation rather than factual data\"\"\"\n",
        "    query_lower = query.lower()\n",
        "    return any(re.search(pattern, query_lower) for pattern in EXPLANATORY_PATTERNS)\n",
        "\n",
        "\n",
        "# A confidence score is computed using FAISS and BM25 ranking\n",
        "# FAISS: The similarity score between the response and the retrieved chunks are normalized\n",
        "# BM25: The BM25 scores for the query and response combined tokens is normalized\n",
        "# The mean of top token probability mean and 1-entropy score is the model_conf_signal\n",
        "# FAISS, BM25 and the model_conf_signal are combined using a weighted sum\n",
        "def compute_response_confidence(\n",
        "    query,\n",
        "    response,\n",
        "    retrieved_chunks,\n",
        "    bm25,\n",
        "    model_conf_signal,\n",
        "    lambda_faiss=0.6,\n",
        "    lambda_conf=0.3,\n",
        "    lambda_bm25=1.0,\n",
        "    future_penalty=-0.3,\n",
        "    explanation_penalty=-0.2,\n",
        "):\n",
        "    \"\"\"Calculates a confidence score for the model response\"\"\"\n",
        "    if not retrieved_chunks:\n",
        "        return 0.0\n",
        "    # Compute FAISS similarity\n",
        "    retrieved_embedding = embed_model.encode(\n",
        "        \" \".join(retrieved_chunks), normalize_embeddings=True\n",
        "    )\n",
        "    response_embedding = embed_model.encode(response, normalize_embeddings=True)\n",
        "    faiss_score = np.dot(retrieved_embedding, response_embedding)\n",
        "    # Normalize the FAISS score\n",
        "    normalized_faiss = (faiss_score + 1) / 2\n",
        "    # Compute BM25 for combined query + response\n",
        "    tokenized_combined = (query + \" \" + response).lower().split()\n",
        "    bm25_scores = bm25.get_scores(tokenized_combined)\n",
        "    # Normalize the BM25 score\n",
        "    if bm25_scores.size > 0:\n",
        "        bm25_score = np.mean(bm25_scores)\n",
        "        min_bm25, max_bm25 = np.min(bm25_scores), np.max(bm25_scores)\n",
        "        normalized_bm25 = (\n",
        "            (bm25_score - min_bm25) / (max_bm25 - min_bm25 + 1e-6)\n",
        "            if min_bm25 != max_bm25\n",
        "            else 0\n",
        "        )\n",
        "        normalized_bm25 = max(0, min(1, normalized_bm25))\n",
        "    else:\n",
        "        normalized_bm25 = 0.0\n",
        "    # Penalize if query contains future years\n",
        "    future_penalty = -0.3 if contains_future_year(query, retrieved_chunks) else 0.0\n",
        "    # Penalize if query is reasoning based\n",
        "    explanation_penalty_value = (\n",
        "        explanation_penalty if is_explanatory_query(query) else 0.0\n",
        "    )\n",
        "    logger.info(\n",
        "        f\"Faiss score: {normalized_faiss}, BM25: {normalized_bm25}\\n\"\n",
        "        f\"Mean Top Token + 1-Entropy Avg: {model_conf_signal}\\n\"\n",
        "        f\"Future penalty: {future_penalty}, Reasoning penalty: {explanation_penalty_value}\"\n",
        "    )\n",
        "    # Weighted sum of all the normalized scores\n",
        "    confidence_score = (\n",
        "        lambda_faiss * normalized_faiss\n",
        "        + model_conf_signal * lambda_conf\n",
        "        + lambda_bm25 * normalized_bm25\n",
        "        + future_penalty\n",
        "        + explanation_penalty_value\n",
        "    )\n",
        "    return round(min(100, max(0, confidence_score.item() * 100)), 2)\n",
        "\n",
        "\n",
        "# UI handle for query model button\n",
        "# Loads the saved FAISS embeddings and tokenized chunks for BM25\n",
        "# Check the query for any policy violation\n",
        "# Retrieve similar texts using the RAG implementation\n",
        "# Prompt the loaded SLM along with the retrieved texts and compute confidence score\n",
        "def query_model(\n",
        "    query,\n",
        "    top_k=10,\n",
        "    lambda_faiss=0.5,\n",
        "    repetition_penalty=1.5,\n",
        "    max_new_tokens=100,\n",
        "    use_extraction=False,\n",
        "):\n",
        "    \"\"\"Query function\"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    # Check if FAISS and BM25 indexes exist\n",
        "    if not os.path.exists(\"data/faiss_index.bin\") or not os.path.exists(\n",
        "        \"data/bm25_data.pkl\"\n",
        "    ):\n",
        "        logger.error(\"No index found! Prompting user to upload PDFs.\")\n",
        "        return (\n",
        "            \"Index files not found! Please upload PDFs first to generate embeddings.\",\n",
        "            \"Error\",\n",
        "        )\n",
        "    allowed, reason = is_query_allowed(query)\n",
        "    if not allowed:\n",
        "        logger.error(f\"Query Rejected: {reason}\")\n",
        "        return f\"Query Rejected: {reason}\", \"Warning\"\n",
        "    logger.info(\n",
        "        f\"Received query: {query} | Top-K: {top_k}, \"\n",
        "        f\"Lambda: {lambda_faiss}, Tokens: {max_new_tokens}\"\n",
        "    )\n",
        "    # Load FAISS & BM25 Indexes\n",
        "    index = faiss.read_index(\"data/faiss_index.bin\")\n",
        "    with open(\"data/bm25_data.pkl\", \"rb\") as f:\n",
        "        bm25_data = pickle.load(f)\n",
        "    # Restore tokenized chunks and metadata\n",
        "    tokenized_chunks = bm25_data[\"tokenized_chunks\"]\n",
        "    chunk_texts = bm25_data[\"chunk_texts\"]\n",
        "    bm25 = BM25Okapi(tokenized_chunks)\n",
        "    retrieved_chunks = hybrid_retrieve(\n",
        "        query, chunk_texts, index, bm25, top_k=top_k, lambda_faiss=lambda_faiss\n",
        "    )\n",
        "    logger.info(\"Retrieved chunks\")\n",
        "    context = \"\"\n",
        "    token_count = 0\n",
        "    # context = \"\\n\".join(retrieved_chunks)\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_tokens = tokenizer(chunk, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
        "        if token_count + chunk_tokens < MAX_CONTEXT_TOKENS:\n",
        "            context += chunk + \"\\n\"\n",
        "            token_count += chunk_tokens\n",
        "        else:\n",
        "            break\n",
        "    prompt = (\n",
        "        \"You are a financial analyst. Answer financial queries concisely using only the numerical data \"\n",
        "        \"explicitly present in the provided financial context:\\n\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        \"Use only the given financial dataâ€”do not assume, infer, or generate missing values.\"\n",
        "        \" Retain the original format of financial figures without conversion.\"\n",
        "        \" If the requested information is unavailable, respond with 'No relevant financial data available.'\"\n",
        "        \" Provide a single-sentence answer without explanations, additional text, or multiple responses.\"\n",
        "        f\"\\nQuery: {query}\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "    logger.info(\"Generating output\")\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    logger.info(f\"Input len: {input_len}\")\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        sequences = output[\"sequences\"][0][input_len:]\n",
        "    execution_time = time.perf_counter() - start_time\n",
        "    logger.info(f\"Query processed in {execution_time:.2f} seconds.\")\n",
        "    # Get the logits per generated token\n",
        "    log_probs = output[\"scores\"]\n",
        "    token_probs = [torch.softmax(lp, dim=-1) for lp in log_probs]\n",
        "    # Extract top token probabilities for each step\n",
        "    token_confidences = [tp.max().item() for tp in token_probs]\n",
        "    # Compute final confidence score\n",
        "    top_token_conf = sum(token_confidences) / len(token_confidences)\n",
        "    print(f\"Token Token Probability Mean: {top_token_conf:.4f}\")\n",
        "    entropy_score = sum(compute_entropy(lp) for lp in log_probs) / len(log_probs)\n",
        "    entropy_conf = 1 - (entropy_score / torch.log(torch.tensor(tokenizer.vocab_size)))\n",
        "    print(f\"Entropy-based Confidence: {entropy_conf:.4f}\")\n",
        "    model_conf_signal = (top_token_conf + (1 - entropy_conf)) / 2\n",
        "    response = tokenizer.decode(sequences, skip_special_tokens=True)\n",
        "    confidence_score = compute_response_confidence(\n",
        "        query, response, retrieved_chunks, bm25, model_conf_signal\n",
        "    )\n",
        "    logger.info(f\"Confidence: {confidence_score}%\")\n",
        "    if confidence_score <= 0.3:\n",
        "        logger.error(f\"The system is unsure about this response.\")\n",
        "        response += \"\\nThe system is unsure about this response.\"\n",
        "    final_out = \"\"\n",
        "    if not use_extraction:\n",
        "        final_out += f\"Context: {context}\\nQuery: {query}\\n\"\n",
        "    final_out += f\"Response: {response}\"\n",
        "    return (\n",
        "        final_out,\n",
        "        f\"Confidence: {confidence_score}%\\nTime taken: {execution_time:.2f} seconds\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(title=\"Financial Statement RAG with LLM\") as ui:\n",
        "    gr.Markdown(\"## Financial Statement RAG with LLM\")\n",
        "    # File upload section\n",
        "    with gr.Group():\n",
        "        gr.Markdown(\"###  Upload & Process Annual Reports\")\n",
        "        file_input = gr.File(\n",
        "            file_count=\"multiple\",\n",
        "            file_types=[\".pdf\", \".csv\"],\n",
        "            type=\"filepath\",\n",
        "            label=\"Upload Annual Reports (PDFs/CSVs)\",\n",
        "        )\n",
        "        process_button = gr.Button(\"Process Files\")\n",
        "        process_output = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
        "    # Query model section\n",
        "    with gr.Group():\n",
        "        gr.Markdown(\"###  Ask a Financial Query\")\n",
        "        query_input = gr.Textbox(label=\"Enter Query\")\n",
        "        with gr.Row():\n",
        "            top_k_input = gr.Number(value=15, label=\"Top K (Default: 15)\")\n",
        "            lambda_faiss_input = gr.Slider(0, 1, value=0.5, label=\"Lambda FAISS (0-1)\")\n",
        "            repetition_penalty = gr.Slider(\n",
        "                1, 2, value=1.2, label=\"Repetition Penality (1-2)\"\n",
        "            )\n",
        "            max_tokens_input = gr.Number(value=100, label=\"Max New Tokens\")\n",
        "        use_extraction = gr.Checkbox(label=\"Retrieve only the answer\", value=False)\n",
        "        query_button = gr.Button(\"Submit Query\")\n",
        "        query_output = gr.Textbox(label=\"Query Response\", interactive=False)\n",
        "        time_output = gr.Textbox(label=\"Time Taken\", interactive=False)\n",
        "    # Button Actions\n",
        "    process_button.click(process_files, inputs=[file_input], outputs=process_output)\n",
        "    query_button.click(\n",
        "        query_model,\n",
        "        inputs=[\n",
        "            query_input,\n",
        "            top_k_input,\n",
        "            lambda_faiss_input,\n",
        "            repetition_penalty,\n",
        "            max_tokens_input,\n",
        "            use_extraction,\n",
        "        ],\n",
        "        outputs=[query_output, time_output],\n",
        "    )\n",
        "\n",
        "# Application entry point\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Gradio server...\")\n",
        "    ui.launch(server_name=\"0.0.0.0\", server_port=7860, pwa=True)\n"
      ]
    }
  ]
}